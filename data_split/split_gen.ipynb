{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from batchgenerators.utilities.file_and_folder_operations import load_json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a raw dir we only use to get the scanner name as nnUNet crops the filepath\n",
    "t1_dir = \"/gscratch/nrdg/asagil/raw_data/t1\"\n",
    "vessel_dir = \"/gscratch/nrdg/asagil/raw_data/vessel\"\n",
    "\n",
    "def get_matched_ids(dirs, split_char=\"-\"):\n",
    "    \"\"\"\n",
    "    returns a sorted set of all ids that exist in all given dirs\n",
    "    \"\"\"\n",
    "    files = [os.listdir(dir) for dir in dirs]\n",
    "    file_ids = [[file.split(split_char)[0] for file in file_list] for\n",
    "                file_list in files]\n",
    "    sets = [set(file_id) for file_id in file_ids]\n",
    "    matched = set.intersection(*sets)\n",
    "    return sorted(matched)\n",
    "\n",
    "\n",
    "def get_filepath_list_from_id(dir, id):\n",
    "    dir_files = os.listdir(dir)\n",
    "    out_paths = []\n",
    "    for file in dir_files:\n",
    "        if id in file:\n",
    "            out_paths.append(os.path.join(dir, file))\n",
    "    return out_paths\n",
    "\n",
    "def get_filename_list_from_id(dir, id):\n",
    "    dir_files = os.listdir(dir)\n",
    "    file_names = []\n",
    "    for file in dir_files:\n",
    "        if id in file:\n",
    "            file_names.append(file)\n",
    "    return file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IOP: 65\n",
      "HH: 176\n",
      "Guys: 293\n",
      "Total: 534\n",
      "Total: 534\n"
     ]
    }
   ],
   "source": [
    "# all ids unsorted\n",
    "ids = get_matched_ids([t1_dir, vessel_dir])\n",
    "\n",
    "# now we sort by the three scanners\n",
    "IOP_ids = []\n",
    "HH_ids = []\n",
    "Guys_ids = []\n",
    "\n",
    "\n",
    "for id in ids:\n",
    "    file_names = get_filename_list_from_id(t1_dir, id)\n",
    "    if len(file_names) != 1:\n",
    "        print(f\"ID {id} has {len(file_names)} files in t1 dir\")\n",
    "        continue\n",
    "    else:\n",
    "        if \"IOP\" in file_names[0]:\n",
    "            IOP_ids.append(id)\n",
    "        elif \"HH\" in file_names[0]:\n",
    "            HH_ids.append(id)\n",
    "        elif \"Guys\" in file_names[0]:\n",
    "            Guys_ids.append(id)\n",
    "\n",
    "print(f\"IOP: {len(IOP_ids)}\")\n",
    "print(f\"HH: {len(HH_ids)}\")\n",
    "print(f\"Guys: {len(Guys_ids)}\")\n",
    "\n",
    "print(f\"Total: {len(IOP_ids) + len(HH_ids) + len(Guys_ids)}\")\n",
    "print(f\"Total: {len(ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_train_test(ids, test_percent=0.2):\n",
    "    num_test = int(len(ids) * test_percent)\n",
    "    test_ids = random.sample(ids, num_test)\n",
    "    train_ids = [id for id in ids if id not in test_ids]\n",
    "    return train_ids, test_ids\n",
    "\n",
    "IOP_train_ids, IOP_test_ids = get_train_test(IOP_ids)\n",
    "HH_train_ids, HH_test_ids = get_train_test(HH_ids)\n",
    "Guys_train_ids, Guys_test_ids = get_train_test(Guys_ids)\n",
    "\n",
    "assert(len(IOP_train_ids) + len(IOP_test_ids) == len(IOP_ids))\n",
    "assert(len(HH_train_ids) + len(HH_test_ids) == len(HH_ids))\n",
    "assert(len(Guys_train_ids) + len(Guys_test_ids) == len(Guys_ids))\n",
    "\n",
    "## we now have are train and test ids for each scanner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_5_folds(ids):\n",
    "    random.shuffle(ids)\n",
    "    fold_size = len(ids) // 5\n",
    "    remainder = len(ids) % 5\n",
    "\n",
    "    folds = []\n",
    "    start = 0\n",
    "    for i in range(5):\n",
    "        end = start + fold_size + (1 if i < remainder else 0)\n",
    "        folds.append(ids[start:end])\n",
    "        start = end\n",
    "\n",
    "    return folds\n",
    "\n",
    "IOP_folds = get_5_folds(IOP_train_ids)\n",
    "HH_folds = get_5_folds(HH_train_ids)\n",
    "Guys_folds = get_5_folds(Guys_train_ids)\n",
    "\n",
    "assert(len(IOP_folds) == 5)\n",
    "assert(len(HH_folds) == 5)\n",
    "assert(len(Guys_folds) == 5)\n",
    "\n",
    "assert(sum([len(f) for f in IOP_folds]) == len(IOP_train_ids))\n",
    "assert(sum([len(f) for f in HH_folds]) == len(HH_train_ids))\n",
    "assert(sum([len(f) for f in Guys_folds]) == len(Guys_train_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 106\n",
      "Train + Val: 428\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "final_out = []\n",
    "\n",
    "for i in range(5):\n",
    "    out = {}\n",
    "    out[\"val\"] = IOP_folds[i] + HH_folds[i] + Guys_folds[i]\n",
    "    out[\"train\"] = IOP_folds[:i] + IOP_folds[i+1:] + HH_folds[:i] + HH_folds[i+1:] + Guys_folds[:i] + Guys_folds[i+1:]\n",
    "\n",
    "    out[\"train\"] = [item for sublist in out[\"train\"] for item in sublist]\n",
    "    final_out.append(out)\n",
    "    # Save final_out as JSON\n",
    "    with open('splits_final_manual.json', 'w') as f:\n",
    "        json.dump(final_out, f, indent=4)\n",
    "\n",
    "\n",
    "final_test = IOP_test_ids + HH_test_ids + Guys_test_ids\n",
    "final_train_val = [sub_id for sub_id in ids if sub_id not in final_test]\n",
    "\n",
    "assert(len(final_test) + len(final_train_val) == len(ids))\n",
    "print(f'Test: {len(final_test)}')\n",
    "print(f'Train + Val: {len(final_train_val)}')\n",
    "\n",
    "# Save final_test as JSON\n",
    "with open('test_ids.json', 'w') as f:\n",
    "    json.dump(final_test, f, indent=4)\n",
    "\n",
    "# Save final_train_val as JSON\n",
    "with open('train_val_ids.json', 'w') as f:\n",
    "    json.dump(final_train_val, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we have all our splits, we setup nnUNET\n",
    "\n",
    "\n",
    "# set enviroment vars\n",
    "os.environ[\"nnUNet_raw\"] = \"/gscratch/nrdg/asagil/nnUNET_data/nnunet_raw\"\n",
    "os.environ[\"nnUNet_preprocessed\"] = \"/gscratch/nrdg/asagil/nnUNET_data/nnunet_preprocessed\"\n",
    "os.environ[\"nnUNet_results\"] = \"/gscratch/nrdg/asagil/nnUNET_data/nnunet_results\"\n",
    "\n",
    "from nnunetv2.paths import nnUNet_raw\n",
    "from batchgenerators.utilities.file_and_folder_operations import *\n",
    "from nnunetv2.dataset_conversion.generate_dataset_json import generate_dataset_json\n",
    "from tqdm import tqdm\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw data folder: /gscratch/nrdg/asagil/nnUNET_data/nnunet_raw\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "IXI_t1_dir = \"/gscratch/nrdg/asagil/raw_data/t1\"\n",
    "IXI_vessel_dir = \"/gscratch/nrdg/asagil/raw_data/costa\"\n",
    "\n",
    "task_id = 95\n",
    "task_name = \"IXI-costa-even-split\"\n",
    "\n",
    "foldername = \"Dataset%03.0d_%s\" % (task_id, task_name)\n",
    "\n",
    "# setting up nnU-Net folders\n",
    "print(f'raw data folder: {nnUNet_raw}')\n",
    "\n",
    "\n",
    "out_base = join(nnUNet_raw, foldername)\n",
    "imagestr = join(out_base, \"imagesTr\")\n",
    "imagests = join(out_base, \"imagesTs\")\n",
    "labelstr = join(out_base, \"labelsTr\")\n",
    "labelsts = join(out_base, \"labelsTs\")\n",
    "\n",
    "\n",
    "maybe_mkdir_p(imagestr)\n",
    "maybe_mkdir_p(labelstr)\n",
    "maybe_mkdir_p(imagests)\n",
    "maybe_mkdir_p(labelsts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 106/106 [00:13<00:00,  7.64it/s]\n",
      "100%|██████████| 428/428 [02:10<00:00,  3.27it/s]\n"
     ]
    }
   ],
   "source": [
    "for sub_id in tqdm(final_test):\n",
    "    t1_path = get_filepath_list_from_id(t1_dir, sub_id)\n",
    "    vessel_path = get_filepath_list_from_id(vessel_dir, sub_id)\n",
    "    if len(t1_path) != 1 or len(vessel_path) != 1:\n",
    "        print(f\"ID {sub_id} has {len(t1_path)} t1 files and {len(vessel_path)} vessel files\")\n",
    "        continue\n",
    "    else:\n",
    "        t1_path = t1_path[0]\n",
    "        vessel_path = vessel_path[0]\n",
    "\n",
    "    shutil.copy(t1_path, join(imagests, f\"{sub_id}_0000.nii.gz\"))\n",
    "    shutil.copy(vessel_path, join(labelsts, f\"{sub_id}.nii.gz\"))\n",
    "\n",
    "for sub_id in tqdm(final_train_val):\n",
    "    t1_path = get_filepath_list_from_id(t1_dir, sub_id)\n",
    "    vessel_path = get_filepath_list_from_id(vessel_dir, sub_id)\n",
    "    if len(t1_path) != 1 or len(vessel_path) != 1:\n",
    "        print(f\"ID {sub_id} has {len(t1_path)} t1 files and {len(vessel_path)} vessel files\")\n",
    "        continue\n",
    "    else:\n",
    "        t1_path = t1_path[0]\n",
    "        vessel_path = vessel_path[0]\n",
    "\n",
    "    shutil.copy(t1_path, join(imagestr, f\"{sub_id}_0000.nii.gz\"))\n",
    "    shutil.copy(vessel_path, join(labelstr, f\"{sub_id}.nii.gz\"))\n",
    "\n",
    "generate_dataset_json(out_base,\n",
    "                    channel_names={0: 'T1'},\n",
    "                    labels={\n",
    "                        'background': 0,\n",
    "                        'vessel': 1,\n",
    "                    },\n",
    "                    num_training_cases=(len(final_train_val)),\n",
    "                    file_ending='.nii.gz',\n",
    "                    license='see https://www.synapse.org/#!Synapse:syn25829067/wiki/610863',\n",
    "                    reference='see https://www.synapse.org/#!Synapse:syn25829067/wiki/610863',\n",
    "                    dataset_release='1.0')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_nnunet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
